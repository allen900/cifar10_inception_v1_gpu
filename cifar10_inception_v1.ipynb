{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cifar10_inception_v1.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "NZ7kzKKVrxoz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import datetime\n",
        "\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "from keras.regularizers import l1,l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from keras.models import model_from_json, Model\n",
        "\n",
        "from keras.layers import Flatten, Activation, Conv2D, MaxPool2D, AvgPool2D, Dense, Dropout, BatchNormalization, Input, MaxPooling2D, Flatten, Activation, Conv2D, AvgPool2D, Dense, Dropout, concatenate, AveragePooling2D\n",
        "\n",
        "np.random.seed(161)\n",
        "\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train = x_train / 255.0\n",
        "x_test = x_test / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dYkCosxDwbhR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#选取彩色通道，将图片转换为灰度图\n",
        "x_train_gray = np.dot(x_train[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "x_test_gray = np.dot(x_test[:,:,:,:3], [0.299, 0.587, 0.114])\n",
        "\n",
        "#大小统一为32*32像素\n",
        "x_train_gray = x_train_gray.reshape(-1,32,32,1)\n",
        "x_test_gray = x_test_gray.reshape(-1,32,32,1)\n",
        "\n",
        "from keras.utils.np_utils import to_categorical\n",
        "\n",
        "y_train_cat = to_categorical(y_train)\n",
        "y_test_cat = to_categorical(y_test)\n",
        "\n",
        "plt.imshow(x_train[1])\n",
        "#plt.show()\n",
        "\n",
        "plt.imshow(x_train_gray[1,:,:,0], cmap='gray')\n",
        "#plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-25G2n0kx4ti",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_dense(input_layer, neurons_nr, dense_nr, \n",
        "                dropout=False, normalization=False, regularization='l2', dropout_ratio=0.5):\n",
        "  \n",
        "    dense = Dense(neurons_nr, kernel_regularizer=regularization, \n",
        "                  name='dense_%d_%d'%(dense_nr, neurons_nr))(input_layer)\n",
        "    \n",
        "    # 视条件而定 使用dropout/normalization\n",
        "    if dropout:\n",
        "        dense = Dropout(dropout_ratio, name='dense_%d_%ddrop'%(dense_nr, neurons_nr))(dense)\n",
        "    if normalization:\n",
        "        dense = BatchNormalization(name='dense_%d_%dnorm'%(dense_nr, neurons_nr))(dense) #Batch Normalization批量标准化\n",
        "    \n",
        "    return dense\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dP8PlcGv_zhs",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def build_inception_module(input_layer, features_nr, module_nr, \n",
        "                           dropout=False, normalization=False, regularization='l2', dropout_ratio=0.2): \n",
        "  \n",
        "    # feature_nr 是一个用来构建一个inception内部网络层的数组\n",
        "    # 其数据形式为: [1x1, 3x3 reduce, 3x3, 5x5 reduce, 5x5, pool proj]\n",
        "    \n",
        "    # 1*1 卷积核  \n",
        "    inception_1x1 = Conv2D(features_nr[0],1,1,border_mode='same',activation='relu',name='inception_%d_/1x1'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
        "    \n",
        "    # 1. 实现跨通道的交互和信息整合；2. 进行卷积核通道数的降维\n",
        "    inception_3x3_reduce = Conv2D(features_nr[1],1,1,border_mode='same',activation='relu',name='inception_%d_/3x3_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer) \n",
        "    \n",
        "    # 3*3 卷积核\n",
        "    inception_3x3 = Conv2D(features_nr[2],3,3,border_mode='same',activation='relu',name='inception_%d_/3x3'%(module_nr),W_regularizer=l2(0.0002))(inception_3x3_reduce)\n",
        "    \n",
        "    # 1. 实现跨通道的交互和信息整合；2. 进行卷积核通道数的降维\n",
        "    inception_5x5_reduce = Conv2D(features_nr[3],1,1,border_mode='same',activation='relu',name='inception_%d_/5x5_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
        "    \n",
        "    # 5*5 卷积核\n",
        "    inception_5x5 = Conv2D(features_nr[4],5,5,border_mode='same',activation='relu',name='inception_%d_/5x5'%(module_nr),W_regularizer=l2(0.0002))(inception_5x5_reduce)\n",
        "    \n",
        "    # max pooling 核\n",
        "    inception_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_%d_/pool'%(module_nr))(input_layer)\n",
        "    \n",
        "    # 1. 实现跨通道的交互和信息整合；2. 进行卷积核通道数的降维\n",
        "    inception_pool_proj = Conv2D(features_nr[5],1,1,border_mode='same',activation='relu',name='inception_%d_/pool_proj'%(module_nr),W_regularizer=l2(0.0002))(inception_pool)\n",
        "    \n",
        "    # inception 输出\n",
        "    inception_output = concatenate([inception_1x1,inception_3x3,inception_5x5,inception_pool_proj],axis=3,name='inception_%d_/output'%(module_nr))\n",
        "\n",
        "    # 视条件而定 使用dropout/normalization\n",
        "    if dropout:\n",
        "        inception_output = Dropout(dropout_ratio, name='inception_%d_/output_drop'%(module_nr))(inception_output)\n",
        "    if normalization:\n",
        "        inception_output = BatchNormalization(name='inception_%d_/output_norm'%(module_nr))(inception_output)\n",
        "    \n",
        "    # maxpooling层最终输出（2*2）\n",
        "    pooled = MaxPooling2D((2,2), padding='same', name='inception_%d_2x2subsample'%(module_nr))(inception_output)\n",
        "    \n",
        "    return pooled\n",
        " "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "htj6Dk1-_2Zx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#模型名称\n",
        "i='cifar10-nrcrt7-'+datetime.datetime.now().strftime(\"%I:%M%p_%B-%d-%Y\")\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "#在Google云盘创建储存模型与日志的文件夹（工作目录下创建）\n",
        "!mkdir -p models\n",
        "!mkdir -p logs\n",
        "\n",
        "a = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')#如果验证集loss值连续10个周期不下降，程序自动停止（早停法）\n",
        "b = ModelCheckpoint(monitor='val_loss', filepath='./models/'+str(i)+'.hdf5', verbose=1, save_best_only=True)#每个训练周期后，验证集loss值如果下降，则储存改模型（最终只储存最好的模型）\n",
        "c = TensorBoard(log_dir='./logs/'+str(i),\n",
        "                write_grads=True,\n",
        "                write_graph=True,\n",
        "                write_images=True,\n",
        "                batch_size=256)#保存日志文件至Google云盘中\n",
        "\n",
        "#回调函数：当评价指标（验证集loss值）不在提升时，减少学习率 （loss值连续patience次没有变化时，学习率缩小为factor倍）\n",
        "d = ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=4, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "\n",
        "callbacks=[a,b,c,d]\n",
        "\n",
        "#------------模型定义-------------------\n",
        "\n",
        "use_norm = True #使用BN\n",
        "lrate = 0.001 #学习率\n",
        "\n",
        "input_img = Input(shape = (32, 32, 3), name='input') #数据输入\n",
        "\n",
        "inception_1 = build_inception_module(input_img, [64,96,128,16,32,32], 1, False, use_norm) #inception_1\n",
        "\n",
        "inception_2 = build_inception_module(inception_1, [128,128,192,32,96,64], 2, False, use_norm)#inception_2\n",
        "\n",
        "inception_3 = build_inception_module(inception_2, [192,96,208,16,48,64], 3, False, use_norm)#inception_3\n",
        "\n",
        "inception_4 = build_inception_module(inception_3, [160, 112, 224, 24, 64, 64], 4, False, use_norm)#inception_4\n",
        "\n",
        "flat_pool = AveragePooling2D(pool_size=(2, 2), padding='valid')(inception_4) #平均池化\n",
        "\n",
        "flat = Flatten()(flat_pool)\n",
        "\n",
        "dense_5 = build_dense(flat, 128, 1, True, use_norm) # 全连接层\n",
        "\n",
        "dense_6 = build_dense(dense_5, 64, 2, True, use_norm) # 全连接层\n",
        "\n",
        "out = Dense(10, activation='softmax')(dense_6) # 最后一层使用softmax激活函数\n",
        "\n",
        "model = Model(inputs = input_img, outputs = out)# 输出\n",
        "\n",
        "#-----------------------------------------------\n",
        "\n",
        "model.compile(loss='binary_crossentropy', #二分类的损失函数\n",
        "              optimizer=Adam(lrate),\n",
        "              metrics=['accuracy']) #设置损失函数和优化器\n",
        "\n",
        "model.summary()\n",
        "\n",
        "#将模型转换为json文件\n",
        "model_json = model.to_json()\n",
        "with open(\"./models/\"+str(i)+\".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "print(\"已将模型储存至\" + \"../models/\"+str(i)+\".json\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDiGRrfc_-tc",
        "colab_type": "code",
        "outputId": "c309b5a6-21c2-4f7e-bb01-f3f468606a95",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        }
      },
      "source": [
        "def get_model_memory_usage(batch_size, model):\n",
        "    import numpy as np\n",
        "    from keras import backend as K\n",
        "\n",
        "    shapes_mem_count = 0\n",
        "    for l in model.layers:\n",
        "        single_layer_mem = 1\n",
        "        for s in l.output_shape:\n",
        "            if s is None:\n",
        "                continue\n",
        "            single_layer_mem *= s\n",
        "        shapes_mem_count += single_layer_mem\n",
        "\n",
        "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
        "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
        "\n",
        "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
        "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
        "    return gbytes\n",
        "  \n",
        "print(\"内存使用 (GB):\", get_model_memory_usage(128,model))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "内存使用 (GB): 1.47\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENzcS1oeAkcn",
        "colab_type": "code",
        "outputId": "9eb6c32b-0ad3-40d7-aeea-c72a4633e2bf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 3889
        }
      },
      "source": [
        "with tf.device('/gpu:0'):\n",
        "  model.fit(x_train, y_train_cat, batch_size=256, epochs=100, validation_split=0.2,verbose=1,callbacks=callbacks)  # 开始训练 100个周期\n",
        "\n",
        "  \n",
        "  \n",
        "result = model.evaluate(x_test, y_test_cat)\n",
        "\n",
        "print(\"准确率（测试集）: \",result[1]*100,\"%\")\n",
        "\n",
        "#将模型与日志拷贝至Google云盘\n",
        "!cp -R models ./\n",
        "!cp -R logs ./\n",
        "\n",
        "print(\"已将模型与日志拷贝至Google云盘\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 40000 samples, validate on 10000 samples\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "W0618 05:15:22.387765 139696516315008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:850: The name tf.summary.merge_all is deprecated. Please use tf.compat.v1.summary.merge_all instead.\n",
            "\n",
            "W0618 05:15:22.388892 139696516315008 deprecation_wrapper.py:119] From /usr/local/lib/python3.6/dist-packages/keras/callbacks.py:853: The name tf.summary.FileWriter is deprecated. Please use tf.compat.v1.summary.FileWriter instead.\n",
            "\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "40000/40000 [==============================] - 36s 909us/step - loss: 1.3538 - acc: 0.9055 - val_loss: 0.6102 - val_acc: 0.9016\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 0.61020, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 2/100\n",
            "40000/40000 [==============================] - 29s 714us/step - loss: 0.4242 - acc: 0.9287 - val_loss: 0.4597 - val_acc: 0.9018\n",
            "\n",
            "Epoch 00002: val_loss improved from 0.61020 to 0.45972, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 3/100\n",
            "40000/40000 [==============================] - 29s 716us/step - loss: 0.3029 - acc: 0.9411 - val_loss: 0.3234 - val_acc: 0.9250\n",
            "\n",
            "Epoch 00003: val_loss improved from 0.45972 to 0.32344, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 4/100\n",
            "40000/40000 [==============================] - 29s 722us/step - loss: 0.2501 - acc: 0.9480 - val_loss: 0.2651 - val_acc: 0.9384\n",
            "\n",
            "Epoch 00004: val_loss improved from 0.32344 to 0.26511, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 5/100\n",
            "40000/40000 [==============================] - 29s 724us/step - loss: 0.2205 - acc: 0.9532 - val_loss: 0.2765 - val_acc: 0.9314\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 0.26511\n",
            "Epoch 6/100\n",
            "40000/40000 [==============================] - 29s 727us/step - loss: 0.2025 - acc: 0.9574 - val_loss: 0.2763 - val_acc: 0.9300\n",
            "\n",
            "Epoch 00006: val_loss did not improve from 0.26511\n",
            "Epoch 7/100\n",
            "40000/40000 [==============================] - 29s 729us/step - loss: 0.1900 - acc: 0.9606 - val_loss: 0.2542 - val_acc: 0.9352\n",
            "\n",
            "Epoch 00007: val_loss improved from 0.26511 to 0.25420, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 8/100\n",
            "40000/40000 [==============================] - 29s 731us/step - loss: 0.1831 - acc: 0.9621 - val_loss: 0.2703 - val_acc: 0.9353\n",
            "\n",
            "Epoch 00008: val_loss did not improve from 0.25420\n",
            "Epoch 9/100\n",
            "40000/40000 [==============================] - 29s 731us/step - loss: 0.1777 - acc: 0.9640 - val_loss: 0.2535 - val_acc: 0.9353\n",
            "\n",
            "Epoch 00009: val_loss improved from 0.25420 to 0.25349, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 10/100\n",
            "40000/40000 [==============================] - 29s 732us/step - loss: 0.1694 - acc: 0.9665 - val_loss: 0.2083 - val_acc: 0.9511\n",
            "\n",
            "Epoch 00010: val_loss improved from 0.25349 to 0.20834, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 11/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.1646 - acc: 0.9680 - val_loss: 0.2523 - val_acc: 0.9343\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 0.20834\n",
            "Epoch 12/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1656 - acc: 0.9683 - val_loss: 0.2143 - val_acc: 0.9496\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 0.20834\n",
            "Epoch 13/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1569 - acc: 0.9710 - val_loss: 0.2376 - val_acc: 0.9452\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 0.20834\n",
            "Epoch 14/100\n",
            "40000/40000 [==============================] - 29s 736us/step - loss: 0.1544 - acc: 0.9723 - val_loss: 0.2037 - val_acc: 0.9548\n",
            "\n",
            "Epoch 00014: val_loss improved from 0.20834 to 0.20365, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 15/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.1562 - acc: 0.9718 - val_loss: 0.2501 - val_acc: 0.9448\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 0.20365\n",
            "Epoch 16/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.1502 - acc: 0.9737 - val_loss: 0.2265 - val_acc: 0.9493\n",
            "\n",
            "Epoch 00016: val_loss did not improve from 0.20365\n",
            "Epoch 17/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1481 - acc: 0.9748 - val_loss: 0.2081 - val_acc: 0.9527\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 0.20365\n",
            "Epoch 18/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1444 - acc: 0.9758 - val_loss: 0.1986 - val_acc: 0.9564\n",
            "\n",
            "Epoch 00018: val_loss improved from 0.20365 to 0.19863, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 19/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1411 - acc: 0.9771 - val_loss: 0.2441 - val_acc: 0.9412\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 0.19863\n",
            "Epoch 20/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1403 - acc: 0.9773 - val_loss: 0.2729 - val_acc: 0.9394\n",
            "\n",
            "Epoch 00020: val_loss did not improve from 0.19863\n",
            "Epoch 21/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1444 - acc: 0.9767 - val_loss: 0.1966 - val_acc: 0.9582\n",
            "\n",
            "Epoch 00021: val_loss improved from 0.19863 to 0.19660, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 22/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1382 - acc: 0.9786 - val_loss: 0.2122 - val_acc: 0.9519\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 0.19660\n",
            "Epoch 23/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.1364 - acc: 0.9794 - val_loss: 0.2185 - val_acc: 0.9527\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 0.19660\n",
            "Epoch 24/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1376 - acc: 0.9792 - val_loss: 0.1959 - val_acc: 0.9606\n",
            "\n",
            "Epoch 00024: val_loss improved from 0.19660 to 0.19585, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 25/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.1341 - acc: 0.9802 - val_loss: 0.2297 - val_acc: 0.9524\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 0.19585\n",
            "Epoch 26/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.1337 - acc: 0.9804 - val_loss: 0.2012 - val_acc: 0.9588\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 0.19585\n",
            "Epoch 27/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.1319 - acc: 0.9809 - val_loss: 0.2346 - val_acc: 0.9514\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 0.19585\n",
            "Epoch 28/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.1272 - acc: 0.9824 - val_loss: 0.1996 - val_acc: 0.9587\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 0.19585\n",
            "Epoch 29/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0993 - acc: 0.9910 - val_loss: 0.1569 - val_acc: 0.9683\n",
            "\n",
            "Epoch 00029: val_loss improved from 0.19585 to 0.15686, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 30/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.0789 - acc: 0.9956 - val_loss: 0.1495 - val_acc: 0.9683\n",
            "\n",
            "Epoch 00030: val_loss improved from 0.15686 to 0.14950, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 31/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.0698 - acc: 0.9971 - val_loss: 0.1467 - val_acc: 0.9694\n",
            "\n",
            "Epoch 00031: val_loss improved from 0.14950 to 0.14668, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 32/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.0655 - acc: 0.9975 - val_loss: 0.1455 - val_acc: 0.9700\n",
            "\n",
            "Epoch 00032: val_loss improved from 0.14668 to 0.14546, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 33/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0595 - acc: 0.9983 - val_loss: 0.1585 - val_acc: 0.9680\n",
            "\n",
            "Epoch 00033: val_loss did not improve from 0.14546\n",
            "Epoch 34/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0562 - acc: 0.9985 - val_loss: 0.1554 - val_acc: 0.9676\n",
            "\n",
            "Epoch 00034: val_loss did not improve from 0.14546\n",
            "Epoch 35/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0546 - acc: 0.9982 - val_loss: 0.1702 - val_acc: 0.9663\n",
            "\n",
            "Epoch 00035: val_loss did not improve from 0.14546\n",
            "Epoch 36/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0579 - acc: 0.9967 - val_loss: 0.1925 - val_acc: 0.9632\n",
            "\n",
            "Epoch 00036: val_loss did not improve from 0.14546\n",
            "Epoch 37/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0520 - acc: 0.9987 - val_loss: 0.1513 - val_acc: 0.9703\n",
            "\n",
            "Epoch 00037: val_loss did not improve from 0.14546\n",
            "Epoch 38/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0475 - acc: 0.9996 - val_loss: 0.1434 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00038: val_loss improved from 0.14546 to 0.14339, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 39/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.0452 - acc: 0.9998 - val_loss: 0.1427 - val_acc: 0.9705\n",
            "\n",
            "Epoch 00039: val_loss improved from 0.14339 to 0.14273, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 40/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.0433 - acc: 0.9999 - val_loss: 0.1436 - val_acc: 0.9701\n",
            "\n",
            "Epoch 00040: val_loss did not improve from 0.14273\n",
            "Epoch 41/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0417 - acc: 0.9999 - val_loss: 0.1399 - val_acc: 0.9708\n",
            "\n",
            "Epoch 00041: val_loss improved from 0.14273 to 0.13993, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 42/100\n",
            "40000/40000 [==============================] - 29s 733us/step - loss: 0.0402 - acc: 1.0000 - val_loss: 0.1393 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00042: val_loss improved from 0.13993 to 0.13926, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 43/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0389 - acc: 1.0000 - val_loss: 0.1383 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00043: val_loss improved from 0.13926 to 0.13834, saving model to ./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5\n",
            "Epoch 44/100\n",
            "40000/40000 [==============================] - 29s 734us/step - loss: 0.0377 - acc: 0.9999 - val_loss: 0.1429 - val_acc: 0.9700\n",
            "\n",
            "Epoch 00044: val_loss did not improve from 0.13834\n",
            "Epoch 45/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.0367 - acc: 1.0000 - val_loss: 0.1457 - val_acc: 0.9703\n",
            "\n",
            "Epoch 00045: val_loss did not improve from 0.13834\n",
            "Epoch 46/100\n",
            "40000/40000 [==============================] - 29s 736us/step - loss: 0.0359 - acc: 0.9999 - val_loss: 0.1474 - val_acc: 0.9700\n",
            "\n",
            "Epoch 00046: val_loss did not improve from 0.13834\n",
            "Epoch 47/100\n",
            "40000/40000 [==============================] - 29s 736us/step - loss: 0.0345 - acc: 1.0000 - val_loss: 0.1412 - val_acc: 0.9705\n",
            "\n",
            "Epoch 00047: val_loss did not improve from 0.13834\n",
            "Epoch 48/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.0335 - acc: 1.0000 - val_loss: 0.1443 - val_acc: 0.9702\n",
            "\n",
            "Epoch 00048: val_loss did not improve from 0.13834\n",
            "Epoch 49/100\n",
            "40000/40000 [==============================] - 29s 737us/step - loss: 0.0330 - acc: 1.0000 - val_loss: 0.1444 - val_acc: 0.9706\n",
            "\n",
            "Epoch 00049: val_loss did not improve from 0.13834\n",
            "Epoch 50/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.0326 - acc: 1.0000 - val_loss: 0.1430 - val_acc: 0.9704\n",
            "\n",
            "Epoch 00050: val_loss did not improve from 0.13834\n",
            "Epoch 51/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.0322 - acc: 1.0000 - val_loss: 0.1443 - val_acc: 0.9705\n",
            "\n",
            "Epoch 00051: val_loss did not improve from 0.13834\n",
            "Epoch 52/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.0319 - acc: 1.0000 - val_loss: 0.1453 - val_acc: 0.9708\n",
            "\n",
            "Epoch 00052: val_loss did not improve from 0.13834\n",
            "Epoch 53/100\n",
            "40000/40000 [==============================] - 29s 735us/step - loss: 0.0317 - acc: 1.0000 - val_loss: 0.1458 - val_acc: 0.9704\n",
            "\n",
            "Epoch 00053: val_loss did not improve from 0.13834\n",
            "Epoch 00053: early stopping\n",
            "10000/10000 [==============================] - 3s 349us/step\n",
            "准确率（测试集）:  96.87000011444091 %\n",
            "cp: 'models' and './models' are the same file\n",
            "cp: 'logs' and './logs' are the same file\n",
            "已将模型与日志拷贝至Google云盘\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LEOkTpzKotO",
        "colab_type": "code",
        "outputId": "f562a6df-fcc2-48e6-dc81-ff381de0b89c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "model.load_weights('./models/cifar10-nrcrt7-05:15AM_June-18-2019.hdf5')\n",
        "\n",
        "result = model.evaluate(x_test, y_test_cat)      \n",
        "\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 3s 338us/step\n",
            "[0.14873394862413405, 0.9690299997329712]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}